{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/meraline/pyda_homeworks/blob/master/netology_case_study_books_recommend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edQI-m-cCgpV"
   },
   "source": [
    "# Кейс-стади\n",
    "\n",
    "Булыгин Олег:  \n",
    "* [LinkedIn](linkedin.com/in/obulygin)  \n",
    "* [Telegram](https://t.me/obulygin91)   \n",
    "* [Vk](vk.com/obulygin91)  \n",
    "* email: obulygin91@ya.ru  \n",
    "\n",
    "[Сообщество по Python](https://yandex.ru/q/loves/pythontalk/) на Кью  \n",
    "[Сообщество по Data Science и анализу данных](https://yandex.ru/q/loves/datatalk/) на Кью "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ijitb1rl2OjG"
   },
   "source": [
    "# Рекомендательной система книг на примере работ Дарвина"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E32s5x1I2OjM"
   },
   "outputs": [],
   "source": [
    "# импорт библиотек\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBp_AAiC2OjO"
   },
   "outputs": [],
   "source": [
    "# считаем все файлы, которые хранятся в папке (книги)\n",
    "book_files = glob.glob('*.txt')\n",
    "book_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I31AICYz2OjP"
   },
   "outputs": [],
   "source": [
    "# создадим список названий книг и список текстов книг\n",
    "texts = []\n",
    "titles = []\n",
    "\n",
    "for file in book_files:\n",
    "    text = open(file).read()\n",
    "    title = re.search(r'(\\w+)\\.txt', file).group(1)\n",
    "    texts.append(text)\n",
    "    titles.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaygJFAh2OjQ"
   },
   "outputs": [],
   "source": [
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8voUppMM2OjQ"
   },
   "outputs": [],
   "source": [
    "# Предположим, нам очень понравилась книга \"Происхождение видов\". \n",
    "# Какую книгу из всего списка, нам прочитать следующей, которая может понравится?\n",
    "# Какие тексты могут считаться похожими?\n",
    "texts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RavvzhAD2OjR"
   },
   "outputs": [],
   "source": [
    "# приведем все слова к нижнему регистру\n",
    "texts_lower_case = [text.lower() for text in texts]\n",
    "# проверим\n",
    "texts_lower_case[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCvPA0FO2OjS"
   },
   "outputs": [],
   "source": [
    "# уберем все служебные символы\n",
    "print(len(texts_lower_case[5]))\n",
    "only_words_text = [re.sub('[\\W_]+', ' ', text) for text in texts_lower_case]\n",
    "# проверим\n",
    "print(len(only_words_text[5]))\n",
    "only_words_text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-l0jOvC2OjS"
   },
   "outputs": [],
   "source": [
    "# разделим все на слова\n",
    "texts_splitted = [text.split() for text in only_words_text]\n",
    "# сколько слов?\n",
    "len(texts_splitted[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ga_LEKF2OjT"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "without_sw = [[word for word in text if word not in stopwords_set] for text in texts_splitted]\n",
    "len(without_sw[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7GbbBsm2OjU"
   },
   "outputs": [],
   "source": [
    "without_sw[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YFx5Jn92OjU"
   },
   "source": [
    "Произведем лемматизацию\n",
    "\n",
    "Лемматизация — процесс приведения словоформы к лемме — её нормальной (словарной) форме. \n",
    "В русском языке:\n",
    "- для существительных — именительный падеж, единственное число;\n",
    "- для прилагательных — именительный падеж, единственное число, мужской род;\n",
    "- для глаголов, причастий, деепричастий — глагол в инфинитиве несовершенного вида.\n",
    "\n",
    "В других языках – по аналогии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4b4DCgT2OjU"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_text = [[wordnet_lemmatizer.lemmatize(word) for word in text] for text in without_sw]\n",
    "lemmatized_text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRrl35iv2OjV"
   },
   "outputs": [],
   "source": [
    "# https://www.machinelearningplus.com/nlp/gensim-tutorial/\n",
    "# библиотека gensim позволяет эффективно работать с корпусами текстов\n",
    "from gensim import corpora\n",
    "\n",
    "# создадим словарь, в котором присвоем каждому словую свой уникальный id\n",
    "dictionary = corpora.Dictionary(lemmatized_text)\n",
    "dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6W7L_PP62OjW"
   },
   "outputs": [],
   "source": [
    "len(dict(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrMcoEen2OjW"
   },
   "source": [
    "Создадим на основе словаря модель [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model), которая присваивает каждому слову (id) количество его вхождений в конкретном документе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0NjwvVY2OjW"
   },
   "outputs": [],
   "source": [
    "bows = [dictionary.doc2bow(text) for text in lemmatized_text]\n",
    "bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4yOJJeg2OjX"
   },
   "outputs": [],
   "source": [
    "# для примера посмотрим на bow в виде датафрейма для книги \"Происхождене видов\"\n",
    "bow_oos = pd.DataFrame(bows[5])\n",
    "bow_oos.columns = ['index', 'occurrences']\n",
    "bow_oos\n",
    "# почему здесь не все индексы?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtnHxNMr2OjX"
   },
   "outputs": [],
   "source": [
    "# смэтчим индексы с реальными словами\n",
    "bow_oos['word'] = [dictionary[index] for index in bow_oos['index']]\n",
    "bow_oos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xja8hM5U2OjX"
   },
   "source": [
    "Создадим модель [TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF) (term frequency-inverse document frequency – частотность терминов-обратная частотность документов)\n",
    "\n",
    "Если слово встречается в каком-либо документе часто, при этом встречаясь редко во всех остальных документах — это слово имеет большую значимость для исходного документа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yzoCsY22OjY"
   },
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "tfidf = TfidfModel(bows)\n",
    "\n",
    "# посмотрим, как это выглядит на \"Происхождение видов\"\n",
    "tfidf[bows[5]]\n",
    "\n",
    "# а почему здесь не все индексы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eHbkgPw2OjY"
   },
   "source": [
    "Теперь каждый наш текст представлен по-сути вектором чисел. Сходство между ними мы можем определить при помощи [косинусного сходства](https://www.machinelearningplus.com/nlp/cosine-similarity/) между соответствующими векторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQKlutZd2OjY"
   },
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "cos_sim = similarities.MatrixSimilarity(tfidf[bows])\n",
    "# преобразуем в датафрейм\n",
    "cos_sim_df = pd.DataFrame(list(cos_sim))\n",
    "# добавним название текстов\n",
    "cos_sim_df.columns = titles\n",
    "cos_sim_df.index = titles\n",
    "cos_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbHCIdic2OjZ"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cos_sim_oos = cos_sim_df[['OriginofSpecies']].sort_values('OriginofSpecies', ascending=False)\n",
    "cos_sim_oos\n",
    "ax = sns.barplot(y=cos_sim_oos.index, x=cos_sim_oos.OriginofSpecies, orient='h')\n",
    "ax.set_title('Сходство книг относительно \"Происхождение видов\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKiYYmfn2OjZ"
   },
   "source": [
    "Проведем кластеризацию книг при помощи [метода Уорда](https://neerc.ifmo.ru/wiki/index.php?title=%D0%98%D0%B5%D1%80%D0%B0%D1%80%D1%85%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F).  \n",
    "Метод Уорда основан на объединении не максимально близких кластеров, а тех,слияние которых дает наименьший прирост внутрикластерной дисперсии. Этот метод применяется для задач с близко расположенными кластерами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RM4XM7dQ2OjZ"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "clusters = hierarchy.linkage(cos_sim_df, method='ward')\n",
    "\n",
    "dendrogram = hierarchy.dendrogram(clusters, \n",
    "                                  leaf_font_size=11, \n",
    "                                  labels=list(cos_sim_df.index), \n",
    "                                  orientation=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xK_ieWUK2Oja"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "netology_case_study_books_recommend.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
