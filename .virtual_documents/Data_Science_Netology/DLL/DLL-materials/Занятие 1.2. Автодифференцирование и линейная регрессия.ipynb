import torch
from torch import autograd


x = torch.arange(4).type(torch.float)
print(x)


x.requires_grad


x = x.requires_grad_()


x.requires_grad


x.detach().requires_grad


x.grad


y = 2 * x.dot(x.T)
y2 = 2 * x.dot(x.T)


y


y.backward()




x.grad


4 * x == x.grad


y = 2 * x.dot(x)
print(y.grad_fn)
with torch.no_grad():
    y = 2 * x.dot(x)
    print(y.grad_fn)


x.grad


y = 2 * x.dot(x)
y.backward()
print(x.grad)


y = 2 * x.dot(x)
with torch.no_grad():
    y.backward()
    print(x.grad)


def f(a):
    b = a * 2
    while b.norm().item() < 1000:
        b = b * 2
    if b.sum().item() > 0:
        c = b
    else:
        c = 100 * b
    return c


a = torch.randn(1).requires_grad_()
d = f(a)
d.backward()


d


a


from matplotlib import pyplot as plt


X = [w / 100.0 for w in range(-1000,1000,1)]


Y = []
Y_der = []


for x in X:
  xt = torch.Tensor((x,))
  xt.requires_grad_()
  y = torch.sin(xt) * xt
  y.backward()
  Y.append(y.item())
  Y_der.append(xt.grad.item())


plt.plot(X,Y)
plt.plot(X,Y_der)
plt.grid()
plt.show()


import torch
import seaborn as sns

def f(x):
  return torch.sin(x)*x 

x = torch.linspace(-10, 10, 1000).requires_grad_()
y = f(x)

y.backward(torch.ones_like(y))  
dx = x.grad

plot_data_1 = torch.stack([x.detach(), dx], dim=-1).numpy()
plot_data_2 = torch.stack([x.detach(), y.detach()], dim=-1).numpy()

sns.lineplot(data=plot_data_1)
sns.lineplot(data=plot_data_2)

plt.grid()


import seaborn as sns
import torch

def f(x):
  return torch.sin(x)*x

x = torch.linspace(-10, 10, 1000).requires_grad_()
y = f(x)

y2 = y.sum() 
y2.backward()
dx = x.grad

plot_data1 = torch.stack([x.detach(), dx], dim=-1).numpy() 
plot_data2 = torch.stack([x.detach(), y.detach()], dim=-1).numpy()

sns.lineplot(data=plot_data1)
sns.lineplot(data=plot_data2)

plt.grid()


get_ipython().run_line_magic("matplotlib", " inline")
from IPython import display
from matplotlib import pyplot as plt
import torch
import random


num_inputs = 2
num_examples = 1000
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features = torch.randn((num_examples, num_inputs))
labels = torch.mv(features, true_w) + true_b
labels += torch.randn(labels.shape)


plt.figure(figsize=(12, 4))
plt.subplot(131)
plt.scatter(features[:, 0], labels, 1)
plt.subplot(132)
plt.scatter(features[:, 1], labels, 1)
plt.subplot(133)
plt.scatter(features[:, 0], features[:, 1], 1)
plt.show()


import random

def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        j = indices[i: min(i + batch_size, num_examples)]
        yield features[j, :], labels[j]


batch_size = 10
for X, y in data_iter(batch_size, features, labels):
    print(X,'\n', y)
    break


w = torch.randn((num_inputs))
b = torch.zeros((1,))


w.requires_grad_()
b.requires_grad_()


def linreg(X, w, b):
  return torch.mv(X,w)+b


def squared_loss(y_hat, y):
  return ((y_hat-y.reshape(y_hat.shape)) ** 2).mean()


def sgd(params, lr):
  for param in params:
    param.data[:] = param - lr*param.grad
    


lr = 0.01  
num_epochs = 10

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):   
        w = w.detach()
        b = b.detach()
        w.requires_grad_()
        b.requires_grad_()
        
        l = squared_loss(linreg(X,w,b), y)
        l.backward()
        sgd([w,b], lr)
    train_l = squared_loss(linreg(features, w, b), labels)
    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))





print('Error in estimating w', true_w - w.reshape(true_w.shape))
print('Error in estimating b', true_b - b)
print(w)
print(b)


from torch.utils.data import TensorDataset, DataLoader


num_inputs = 2
num_examples = 1000
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features = torch.randn((num_examples, num_inputs))
labels = torch.mv(features, true_w) + true_b
labels += torch.randn(labels.shape)


batch_size = 10
dataset = TensorDataset(features, labels)
# Randomly reading mini-batches
data_iter = DataLoader(dataset, batch_size, shuffle=True)

# Read a batch to see how it works
for X, y in data_iter:
    print(X, y)
    break


X * w + b 


model = torch.nn.Sequential(torch.nn.Linear(2, 1))


model


model[0].weight.data = true_w.clone().detach().requires_grad_(True).reshape((1, 2))
model[0].bias.data = torch.tensor([true_b], requires_grad = True)


loss = torch.nn.MSELoss(reduction='mean')





trainer = torch.optim.SGD(model.parameters(), lr=0.001)


zzz = torch.tensor([[1.,2.],[3.,4.],[5.,6.]])


labels.shape


model(zzz).shape


model(zzz).reshape(-1)


num_epochs = 100
for epoch in range(1, num_epochs + 1):
    for X, y in data_iter:
        trainer.zero_grad()
        l = loss(model(X).reshape(-1), y)
        l.backward()
        trainer.step()
    l = loss(model(features).reshape(-1), labels)
    if epoch % 5 == 0:
        print('epoch %d, loss: %f' % (epoch, l.item()),'|\tw', model[0].weight.data, '|\tb', model[0].bias.data)
    


w = model[0].weight.data
print('Error in estimating w', true_w.reshape(w.shape) - w)
b = model[0].bias.data
print('Error in estimating b', true_b - b)


model[0].weight.data


model[0].bias.data






